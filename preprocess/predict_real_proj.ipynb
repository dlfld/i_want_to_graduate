{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import javalang\n",
    "from javalang.tree import MethodDeclaration\n",
    "\n",
    "\n",
    "def get_method_asts(code: str) -> List[MethodDeclaration]:\n",
    "    \"\"\"\n",
    "        获取java文件内方法AST节点\n",
    "    :param code: .java文件的code\n",
    "    :return: .java文件内所有方法转换成AST的方法节点列表\n",
    "    \"\"\"\n",
    "    method_asts = []\n",
    "    # 将输入代码转换成ast树\n",
    "    program_ast = javalang.parse.parse(code)\n",
    "    # 遍历所有的节点\n",
    "    for ast_type in program_ast.types:\n",
    "        # 节点名\n",
    "        node_name = type(ast_type).__name__\n",
    "        # 方法应该存在于类定义中，因此需要找到类定义的标签\n",
    "        if node_name == \"ClassDeclaration\":\n",
    "            # 遍历类节点的子节点，获取到方法节点\n",
    "            for body in ast_type.body:\n",
    "                # 获取当前节点的节点名\n",
    "                body_type = type(body).__name__\n",
    "                # 如果当前节点是方法节点\n",
    "                if body_type == \"MethodDeclaration\":\n",
    "                    # 添加结果集\n",
    "                    method_asts.append(body)\n",
    "\n",
    "    return method_asts\n",
    "\n",
    "\n",
    "def get_proj_method_asts(proj_dir: str) -> List[MethodDeclaration]:\n",
    "    \"\"\"\n",
    "        扫描工程文件目录，获取工程中所有方法的AST\n",
    "    :param proj_dir: 工程文件目录\n",
    "    :return: 工程中所有的方法ast节点\n",
    "    \"\"\"\n",
    "    method_ast_list = []\n",
    "    # 遍历\n",
    "\n",
    "    for rt, dirs, files in os.walk(proj_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".java\"):\n",
    "                programfile = open(os.path.join(rt, file), encoding='utf-8')\n",
    "                # 读取文件\n",
    "                code = programfile.read()\n",
    "                # 获取方法的AST节点\n",
    "                try:\n",
    "                    asts = get_method_asts(code)\n",
    "                    method_ast_list.extend(asts)\n",
    "                except Exception as e:\n",
    "                    # print(e)\n",
    "                    pass\n",
    "                    \n",
    "                programfile.close()\n",
    "    return method_ast_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dlf/miniconda3/envs/gnn_ast_flow/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "import os\n",
    "import random\n",
    "import javalang\n",
    "import javalang.tree\n",
    "import javalang.ast\n",
    "import javalang.util\n",
    "from javalang.ast import Node\n",
    "import torch\n",
    "from anytree import AnyNode, RenderTree\n",
    "# import treelib\n",
    "from anytree import find\n",
    "from createclone_java import getedge_nextsib, getedge_flow, getedge_nextstmt, getedge_nexttoken, getedge_nextuse\n",
    "import logddd\n",
    "\n",
    "def get_token(node):\n",
    "    \"\"\"\n",
    "        获取输入node的token\n",
    "    \"\"\"\n",
    "    token = ''\n",
    "    if isinstance(node, str):\n",
    "        token = node\n",
    "        # print(f\"node->{node}\")\n",
    "    elif isinstance(node, set):\n",
    "        token = 'Modifier'\n",
    "    elif isinstance(node, Node):\n",
    "        token = node.__class__.__name__\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_child(root):\n",
    "    \"\"\"\n",
    "        获取当前节点的孩子节点\n",
    "    \"\"\"\n",
    "    if isinstance(root, Node):\n",
    "        children = root.children\n",
    "    elif isinstance(root, set):\n",
    "        children = list(root)\n",
    "    else:\n",
    "        children = []\n",
    "\n",
    "    def expand(nested_list):\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                for sub_item in expand(item):\n",
    "                    # print(sub_item)\n",
    "                    yield sub_item\n",
    "            elif item:\n",
    "                # print(item)\n",
    "                yield item\n",
    "\n",
    "    return list(expand(children))\n",
    "\n",
    "\n",
    "def get_sequence(node):\n",
    "    \"\"\"\n",
    "        获取所有的token \n",
    "    \"\"\"\n",
    "    sequence = []\n",
    "    token, children = get_token(node), get_child(node)\n",
    "    sequence.append(token)\n",
    "    for child in children:\n",
    "        res = get_sequence(child)\n",
    "        sequence.extend(res)\n",
    "    return sequence\n",
    "\n",
    "def getnodes(node, nodelist):\n",
    "    nodelist.append(node)\n",
    "    children = get_child(node)\n",
    "    for child in children:\n",
    "        getnodes(child, nodelist)\n",
    "\n",
    "\n",
    "def createtree(root, node, nodelist, parent=None):\n",
    "    id = len(nodelist)\n",
    "    token, children = get_token(node), get_child(node)\n",
    "    if id == 0:\n",
    "        root.token = token\n",
    "        root.data = node\n",
    "    else:\n",
    "        newnode = AnyNode(id=id, token=token, data=node, parent=parent)\n",
    "    nodelist.append(node)\n",
    "    for child in children:\n",
    "        if id == 0:\n",
    "            createtree(root, child, nodelist, parent=root)\n",
    "        else:\n",
    "            createtree(root, child, nodelist, parent=newnode)\n",
    "\n",
    "def getnodeandedge_astonly(node, nodeindexlist, vocabdict, src, tgt):\n",
    "    \"\"\"\n",
    "        创建ast的边\n",
    "        采用的是DFS的方式\n",
    "    \"\"\"\n",
    "    token = node.token\n",
    "    nodeindexlist.append([vocabdict[token]])\n",
    "    for child in node.children:\n",
    "        src.append(node.id)\n",
    "        tgt.append(child.id)\n",
    "        src.append(child.id)\n",
    "        tgt.append(node.id)\n",
    "        getnodeandedge_astonly(child, nodeindexlist, vocabdict, src, tgt)\n",
    "\n",
    "def getnodeandedge(node, nodeindexlist, vocabdict, src, tgt, edgetype):\n",
    "    token = node.token\n",
    "    nodeindexlist.append([vocabdict[token]])\n",
    "    # =====================添加自链接的边   效果不是很好，先不加================================\n",
    "    # src.append(node.id)\n",
    "    # tgt.append(node.id)\n",
    "    # edgetype.append([0])\n",
    "    # =====================添加自链接的边================================\n",
    "    for child in node.children:\n",
    "        src.append(node.id)\n",
    "        tgt.append(child.id)\n",
    "        edgetype.append([0])\n",
    "        src.append(child.id)\n",
    "        tgt.append(node.id)\n",
    "        edgetype.append([0])\n",
    "        getnodeandedge(child, nodeindexlist, vocabdict, src, tgt, edgetype)\n",
    "\n",
    "def countnodes(node, ifcount, whilecount, forcount, blockcount):\n",
    "    token = node.token\n",
    "    if token == 'IfStatement':\n",
    "        ifcount += 1\n",
    "    if token == 'WhileStatement':\n",
    "        whilecount += 1\n",
    "    if token == 'ForStatement':\n",
    "        forcount += 1\n",
    "    if token == 'BlockStatement':\n",
    "        blockcount += 1\n",
    "    # print(ifcount, whilecount, forcount, blockcount)\n",
    "    for child in node.children:\n",
    "        countnodes(child, ifcount, whilecount, forcount, blockcount)\n",
    "\n",
    "def createast(proj_dir):\n",
    "    \"\"\"\n",
    "        遍历工程文件，获取每一个方法的AST，并对AST进行遍历，获取每一个AST的token\n",
    "    \"\"\"\n",
    "    alltokens = []\n",
    "\n",
    "     # 当前工程项目下所有的java方法转换成的ast\n",
    "    proj_method_asts = get_proj_method_asts(proj_dir)\n",
    "    # logddd.log(len(proj_method_asts))\n",
    "\n",
    "    # 遍历每一个方法ast\n",
    "    for method in proj_method_asts:\n",
    "        res = get_sequence(method)\n",
    "        alltokens.extend(res)\n",
    "    \n",
    "    # logddd.log(len(alltokens))\n",
    "\n",
    "    ifcount = 0\n",
    "    whilecount = 0\n",
    "    forcount = 0\n",
    "    blockcount = 0\n",
    "    docount = 0\n",
    "    switchcount = 0\n",
    "    for token in alltokens:\n",
    "        if token == 'IfStatement':\n",
    "            ifcount += 1\n",
    "        if token == 'WhileStatement':\n",
    "            whilecount += 1\n",
    "        if token == 'ForStatement':\n",
    "            forcount += 1\n",
    "        if token == 'BlockStatement':\n",
    "            blockcount += 1\n",
    "        if token == 'DoStatement':\n",
    "            docount += 1\n",
    "        if token == 'SwitchStatement':\n",
    "            switchcount += 1\n",
    "\n",
    "    alltokens = list(set(alltokens))\n",
    "    vocabsize = len(alltokens)\n",
    "    tokenids = range(vocabsize)\n",
    "    vocabdict = dict(zip(alltokens, tokenids))\n",
    "    return proj_method_asts,vocabsize, vocabdict\n",
    "\n",
    "def create_separate_graph(ast_list,vocablen, vocabdict, device, mode='astonly', nextsib=False, ifedge=False,\n",
    "                        whileedge=False, foredge=False, blockedge=False, nexttoken=False, nextuse=False):\n",
    "    \"\"\"\n",
    "        创建图信息，根据ast重新构建树的结构，并在树上添加边\n",
    "        并返回一条处理完成之后的数据\n",
    "    \"\"\"\n",
    "    treelist = []\n",
    "    logddd.log(len(ast_list))\n",
    "    # 遍历方法列表\n",
    "    for tree in ast_list:\n",
    "        nodelist = []\n",
    "        newtree = AnyNode(id=0, token=None, data=None)\n",
    "        # 创建树\n",
    "        createtree(newtree, tree, nodelist)\n",
    "        # print(path)\n",
    "        # print(newtree)\n",
    "        x = []\n",
    "        edgesrc = []\n",
    "        edgetgt = []\n",
    "        edge_attr = []\n",
    "        if mode == 'astonly':\n",
    "            getnodeandedge_astonly(newtree, x, vocabdict, edgesrc, edgetgt)\n",
    "        else:\n",
    "            getnodeandedge(newtree, x, vocabdict, edgesrc, edgetgt, edge_attr)\n",
    "            if nextsib == True:\n",
    "                # 链接下一个兄弟结点，将一个节点连接到它的下一个兄弟姐妹 (从左到右)。\n",
    "                # 因为图神经网络不考虑节点的顺序，所以有必要向我们的神经网络模型提供子的顺序。\n",
    "                # 尝试一下，如果不要兄弟结点呢？不考虑顺序信息。\n",
    "                getedge_nextsib(newtree, vocabdict, edgesrc, edgetgt, edge_attr)\n",
    "            getedge_flow(newtree, vocabdict, edgesrc, edgetgt, edge_attr, ifedge, whileedge, foredge)\n",
    "            if blockedge == True:\n",
    "                getedge_nextstmt(newtree, vocabdict, edgesrc, edgetgt, edge_attr)\n",
    "            tokenlist = []\n",
    "            if nexttoken == True:\n",
    "                getedge_nexttoken(newtree, vocabdict, edgesrc, edgetgt, edge_attr, tokenlist)\n",
    "            variabledict = {}\n",
    "            if nextuse == True:\n",
    "                getedge_nextuse(newtree, vocabdict, edgesrc, edgetgt, edge_attr, variabledict)\n",
    "                edge_index = [edgesrc, edgetgt]\n",
    "\n",
    "        astlength = len(x)\n",
    "        treelist.append([x, edge_index, edge_attr])\n",
    "\n",
    "    return treelist\n",
    "\n",
    "def create_input_model_data(treelist):\n",
    "    \"\"\"\n",
    "        将传入列表的元素进行两两组合，判断他们是不是克隆对\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range(len(treelist)-1):\n",
    "        for j in range(i+1,len(treelist)):\n",
    "            data_list.append([treelist[i][0],treelist[j][0],treelist[i][1],treelist[j][1],treelist[i][2],treelist[j][2],i,j])\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m 3510341103.py:4\tline:4\u001b[0m -> \u001b[32mlogddd.log(len(proj_method_asts))\u001b[0m : \u001b[1;34m (33159,)\u001b[0m Sun Apr 23 19:49:28 2023\n"
     ]
    }
   ],
   "source": [
    "import logddd\n",
    "dirname = 'kafka/'\n",
    "proj_method_asts,vocabsize, vocabdict = createast(dirname)\n",
    "logddd.log(len(proj_method_asts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_separate_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m treelist\u001b[39m=\u001b[39mcreate_separate_graph(proj_method_asts, vocabsize, vocabdict,device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m,mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39melse\u001b[39m\u001b[39m'\u001b[39m,nextsib\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,ifedge\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,whileedge\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,foredge\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,blockedge\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,nexttoken\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,nextuse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m logddd\u001b[39m.\u001b[39mlog(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlen(tree) = \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(treelist)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m proj_data \u001b[39m=\u001b[39m create_input_model_data(treelist)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_separate_graph' is not defined"
     ]
    }
   ],
   "source": [
    "treelist=create_separate_graph(proj_method_asts, vocabsize, vocabdict,device='cpu',mode='else',nextsib=True,ifedge=True,whileedge=True,foredge=True,blockedge=True,nexttoken=True,nextuse=True)\n",
    "logddd.log(f\"len(tree) = {len(treelist)}\")\n",
    "proj_data = create_input_model_data(treelist)\n",
    "print(len(proj_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m 996866222.py:184\tline:184\u001b[0m -> \u001b[32mlogddd.log(len(ast_list))\u001b[0m : \u001b[1;34m (33159,)\u001b[0m Sun Apr 23 20:02:21 2023\n",
      "\u001b[1;33m 3426601332.py:2\tline:2\u001b[0m -> \u001b[32mlogddd.log(f\"len(tree)={len(treelist)}\")\u001b[0m : \u001b[1;34m ('len(tree) = 33159',)\u001b[0m Sun Apr 23 20:02:59 2023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m treelist\u001b[39m=\u001b[39mcreate_separate_graph(proj_method_asts, vocabsize, vocabdict,device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m,mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39melse\u001b[39m\u001b[39m'\u001b[39m,nextsib\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,ifedge\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,whileedge\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,foredge\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,blockedge\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,nexttoken\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,nextuse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m logddd\u001b[39m.\u001b[39mlog(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlen(tree) = \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(treelist)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m proj_data \u001b[39m=\u001b[39m create_input_model_data(treelist)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(proj_data))\n\u001b[1;32m      5\u001b[0m \u001b[39m#加载模型\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 229\u001b[0m, in \u001b[0;36mcreate_input_model_data\u001b[0;34m(treelist)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(treelist)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    228\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(treelist)):\n\u001b[0;32m--> 229\u001b[0m         data_list\u001b[39m.\u001b[39mappend([treelist[i][\u001b[39m0\u001b[39m],treelist[j][\u001b[39m0\u001b[39m],treelist[i][\u001b[39m1\u001b[39m],treelist[j][\u001b[39m1\u001b[39m],treelist[i][\u001b[39m2\u001b[39m],treelist[j][\u001b[39m2\u001b[39;49m],i,j])\n\u001b[1;32m    230\u001b[0m \u001b[39mreturn\u001b[39;00m data_list\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#加载模型\n",
    "model=torch.load('best_network.pth')\n",
    "device=torch.device(\"cuda:0\")\n",
    "model=model.to(device)\n",
    "for data in proj_data:\n",
    "    x1, x2, edge_index1, edge_index2, edge_attr1, edge_attr2,index_i,index_j=data\n",
    "\n",
    "    x1=torch.tensor(x1, dtype=torch.long, device=device)\n",
    "    x2=torch.tensor(x2, dtype=torch.long, device=device)\n",
    "\n",
    "    edge_index1=torch.tensor(edge_index1, dtype=torch.long, device=device)\n",
    "    edge_index2=torch.tensor(edge_index2, dtype=torch.long, device=device)\n",
    "\n",
    "    if edge_attr1!=None:\n",
    "        edge_attr1=torch.tensor(edge_attr1, dtype=torch.long, device=device)\n",
    "        edge_attr2=torch.tensor(edge_attr2, dtype=torch.long, device=device)\n",
    "\n",
    "    data=[x1, x2, edge_index1, edge_index2, edge_attr1, edge_attr2]\n",
    "    logits=model(data)\n",
    "    logits  = logits.squeeze(0)\n",
    "    output = torch.sigmoid(logits)\n",
    "    if output > 0.5:\n",
    "        logddd.log(f\"找到相似 {index_i}  < -- > {index_j} --- > {output}\")\n",
    "    else:\n",
    "        logddd.log(f\" {index_i}  < -- > {index_j} 不相似 --- > {output} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_ast_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
